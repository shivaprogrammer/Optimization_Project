# -*- coding: utf-8 -*-
"""Optimization_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qmYZrA5Z1U_J8O9_xenlsLGWdE_o8FQn

Team Members: Pooja Naveen Poonia, Shivani Tiwari, Suvigya Sharma

Roll Numbers: M24CSA020, M24CSA029, M24CSA033

The Raisin dataset is a popular dataset often used for classification tasks in machine learning. It contains measurements of different geometric features of raisin varieties, and the goal is to classify them into two categories. The dataset is commonly used to build and evaluate classification models based on these features.

Features:

Area: The number of pixels within the boundary of the raisin.

Perimeter: The total length of the boundary of the raisin (in terms of pixels).

MajorAxisLength: The longest line that can be drawn from one point on the boundary of the raisin to another (major axis of the ellipse).

MinorAxisLength: The shortest line that can be drawn from one point on the boundary to another (minor axis of the ellipse).

Eccentricity: A measure of how much the shape of the raisin deviates from being circular, calculated as the ratio of the distance between the foci of the ellipse and the length of the major axis.

ConvexArea: The number of pixels in the smallest convex polygon that can enclose the raisin (convex hull).

Extent: The ratio of the area of the raisin to the area of the bounding box (a rectangular box surrounding the raisin).

Label: The target label representing the class of the raisin (either Kecimen or Besni, the two varieties of raisins).

Target Classes:

Kecimen (0): One variety of raisin.

Besni (1): Another variety of raisin.

**Importing Libraries**
"""

#Import Libraries
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from itertools import product
import warnings
warnings.filterwarnings("ignore")

"""**Loading Dataset**"""

# Load dataset
df = pd.read_csv("Raisin_Dataset.csv")

print("First few rows of the dataset:")
df.head()

"""**Data Preprocessing**"""

# Check for missing values
print("\nMissing Values:")
df.isnull().sum()

# Descriptive Statistics
print("\nDescriptive Statistics:")
df.describe()

df.info()

# Class distribution
print("\nClass distribution:")
print(df['Class'].value_counts())

"""**Exploratoy Data Analysis**"""

# Visualizations
plt.figure(figsize=(12, 6))
sns.countplot(x='Class', data=df)
plt.title('Class Distribution')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

# Histograms of features
df.hist(bins=30, figsize=(15, 10))
plt.suptitle('Histograms of Features')
plt.show()

# Pairplot for feature relationships and class separation
sns.pairplot(df, hue='Class', diag_kind='kde')
plt.suptitle('Pairplot of Features', y=1.02)
plt.show()

# Correlation heatmap
plt.figure(figsize=(12, 8))
numeric_df = df.select_dtypes(include=[np.number])
correlation_matrix = numeric_df.corr()
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True)
plt.title('Correlation Heatmap')
plt.show()

"""**Spiltting Dataset into 70:30 Ratio**"""

# Encoding
df['Class'] = df['Class'].astype('category').cat.codes

df

# Split the data into features and target variable
X = df.drop(columns=['Class']).values
y = df['Class'].values

# Standardize
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

"""**Model Implementation and Evaluation**

**1. Logistic Regression (Scikit-Learn)**
"""

# Normal Logistic Regression (scikit-learn)
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred_sklearn = model.predict(X_test)
accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)

#Accuracy
print(f"Accuracy with Scikit-Learn Logistic Regression: {accuracy_sklearn * 100:.2f}%")

# Print parameters and metrics
print("Intercept (bias):", model.intercept_)
print("Coefficients:", model.coef_)

y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)

print("Predicted labels:", y_pred)
#print("Predicted probabilities:", y_proba)
print("Accuracy:", model.score(X_test, y_test))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("AUC Score:", roc_auc_score(y_test, y_proba[:, 1]))

"""**2. Logistic Regression with Gradient Descent and Backtracking Line Search**"""

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Logistic Regression with Gradient Descent and Backtracking Line Search
def logistic_regression_gd_backtracking(X, y , tol=1e-4):
    m, n = X.shape
    weights = np.zeros(n)
    bias = 0
    accuracy_per_epoch = []
    loss_per_epoch = []
    weights_per_epoch = []
    epoch = 0
    lr = 0.1
    alpha = lr
    backtrack_factor = 0.8
    backtracking_count = 0  # Counter for number of step size adjustments

    while True:
        # Linear model
        linear_model = np.dot(X, weights) + bias
        y_pred = sigmoid(linear_model)

        # Gradient calculation
        dw = (1 / m) * np.dot(X.T, (y_pred - y))
        db = (1 / m) * np.sum(y_pred - y)

        # Loss calculation for tracking
        loss_prev = -np.mean(y * np.log(y_pred + 1e-9) + (1 - y) * np.log(1 - y_pred + 1e-9))
        loss_per_epoch.append(loss_prev)

        # Backtracking line search
        while True:
            new_weights = weights - alpha * dw
            new_bias = bias - alpha * db

            # Calculate new predictions and loss
            linear_model_new = np.dot(X, new_weights) + new_bias
            y_pred_new = sigmoid(linear_model_new)
            loss_new = -np.mean(y * np.log(y_pred_new + 1e-9) + (1 - y) * np.log(1 - y_pred_new + 1e-9))

            if loss_new < loss_prev - 0.01 * alpha * np.linalg.norm(dw)**2:
                break
            else:
                alpha *= backtrack_factor
                backtracking_count += 1  # Increment the count each time alpha is adjusted

        # Update weights and bias
        weights = new_weights
        bias = new_bias
        epoch += 1
        weights_per_epoch.append(weights)

        # Check convergence
        if np.linalg.norm(dw) < tol and abs(db) < tol:
            break

        # Calculate accuracy for the current epoch
        y_pred_class = sigmoid(np.dot(X, weights) + bias) >= 0.5
        accuracy = accuracy_score(y, y_pred_class)
        accuracy_per_epoch.append(accuracy)

    # Print the final learning rate and number of adjustments
    print(f"Final learning rate after backtracking: {alpha}")

    return weights, bias, accuracy_per_epoch, epoch, loss_per_epoch, weights_per_epoch

# Loss curve (Loss vs. Epoch)
def plot_loss(loss_per_epoch):
    plt.plot(loss_per_epoch, label="Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Loss over Epochs(Gradient Descent with Backtracking)")
    plt.legend()
    plt.show()

# Accuracy curve (Accuracy vs. Epoch)
def plot_accuracy(accuracy_per_epoch):
    plt.plot(accuracy_per_epoch, label="Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title("Accuracy over Epochs(Gradient Descent with Backtracking)")
    plt.legend()
    plt.show()

# Weight convergence (Weights vs. Epoch)
def plot_weight_convergence(weights_per_epoch):
    weights_per_epoch = np.array(weights_per_epoch)
    for i in range(weights_per_epoch.shape[1]):
        plt.plot(weights_per_epoch[:, i], label=f"Weight {i}")
    plt.xlabel("Epoch")
    plt.ylabel("Weight Value")
    plt.title("Weight Convergence over Epochs(Gradient Descent with Backtracking)")
    plt.legend()
    plt.show()

# Confusion matrix
def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title("Confusion Matrix(Gradient Descent with Backtracking)")
    plt.show()

"""**Function Calls**"""

# Training with Gradient Descent and Backtracking Line Search
weights_gd_backtrack, bias_gd_backtrack, accuracy_gd_backtrack, epochs_gd_backtrack, loss_gd_backtrack, weights_gd_backtrack_epoch = logistic_regression_gd_backtracking(X_train, y_train)

# Loss curve and Accuracy curve
plot_loss(loss_gd_backtrack)
plot_accuracy(accuracy_gd_backtrack)

# Weight convergence
plot_weight_convergence(weights_gd_backtrack_epoch)

# Print results
print(f"Final Accuracy with Gradient Descent (Backtracking): {accuracy_gd_backtrack[-1] * 100:.2f}% (Iterations: {epochs_gd_backtrack})")
print("Optimized weights:", weights_gd_backtrack)
print("Optimized bias:", bias_gd_backtrack)

# Predictions on test set
y_pred_class = sigmoid(np.dot(X_test, weights_gd_backtrack) + bias_gd_backtrack) >= 0.5
plot_confusion_matrix(y_test, y_pred_class)

""" **3. Logistic Regression with Newton's Method**"""

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Logistic Regression with Newton's Method
def logistic_regression_newton(X, y, tol=1e-4):
    m, n = X.shape
    weights = np.zeros(n)
    bias = 0
    accuracy_per_epoch = []
    loss_per_epoch = []
    weights_per_epoch = []
    epoch = 0

    while True:
        # Linear model
        linear_model = np.dot(X, weights) + bias
        y_pred = sigmoid(linear_model)

        # Gradient and Hessian calculation
        gradient = (1 / m) * np.dot(X.T, (y_pred - y))
        hessian = (1 / m) * (X.T @ np.diag(y_pred * (1 - y_pred)) @ X)

        # Newton's update step
        try:
            weight_update = np.linalg.solve(hessian, gradient)
        except np.linalg.LinAlgError:
            print("Hessian is singular; stopping Newton's method.")
            break

        new_weights = weights - weight_update
        new_bias = bias - np.sum(y_pred - y) / m

        # Loss calculation for tracking
        loss_prev = -np.mean(y * np.log(y_pred + 1e-9) + (1 - y) * np.log(1 - y_pred + 1e-9))
        loss_per_epoch.append(loss_prev)

        # Check convergence
        if np.linalg.norm(new_weights - weights) < tol and abs(new_bias - bias) < tol:
            break

        weights = new_weights
        bias = new_bias
        epoch += 1
        weights_per_epoch.append(weights)

        # Accuracy for the current epoch
        y_pred_class = sigmoid(np.dot(X, weights) + bias) >= 0.5
        accuracy = accuracy_score(y, y_pred_class)
        accuracy_per_epoch.append(accuracy)

    return weights, bias, accuracy_per_epoch, epoch, loss_per_epoch, weights_per_epoch

# Loss curve (Loss vs. Epoch)
def plot_loss(loss_per_epoch):
    plt.plot(loss_per_epoch, label="Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Loss over Epochs (Newton's Method)")
    plt.legend()
    plt.show()

# Accuracy curve (Accuracy vs. Epoch)
def plot_accuracy(accuracy_per_epoch):
    plt.plot(accuracy_per_epoch, label="Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title("Accuracy over Epochs (Newton's Method)")
    plt.legend()
    plt.show()

# Weight convergence (Weights vs. Epoch)
def plot_weight_convergence(weights_per_epoch):
    weights_per_epoch = np.array(weights_per_epoch)
    for i in range(weights_per_epoch.shape[1]):
        plt.plot(weights_per_epoch[:, i], label=f"Weight {i}")
    plt.xlabel("Epoch")
    plt.ylabel("Weight Value")
    plt.title("Weight Convergence over Epochs (Newton's Method)")
    plt.legend()
    plt.show()

# Confusion matrix
def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title("Confusion Matrix (Newton's Method)")
    plt.show()

# Training with Newton's Method
weights_nm, bias_nm, accuracy_nm, epochs_nm, loss_nm, weights_nm_epoch = logistic_regression_newton(X_train, y_train)

# Loss curve and Accuracy curve
plot_loss(loss_nm)
plot_accuracy(accuracy_nm)

# Plot weight convergence (if applicable)
plot_weight_convergence(weights_nm_epoch)

# Print results
print("Optimized weights:", weights_nm)
print("Optimized bias:", bias_nm)
print(f"Final Accuracy with Newton's Method: {accuracy_nm[-1] * 100:.2f}% (Iterations: {epochs_nm})")

# Predictions on test set
y_pred_class = sigmoid(np.dot(X_test, weights_nm) + bias_nm) >= 0.5
plot_confusion_matrix(y_test, y_pred_class)

# Accuracy over epochs
plt.figure(figsize=(10, 6))

plt.plot(range(1, len(accuracy_gd_backtrack) + 1), accuracy_gd_backtrack, label="Gradient Descent (Backtracking)")
plt.plot(range(1, len(accuracy_nm) + 1), accuracy_nm, label="Newton's Method")
plt.axhline(y=accuracy_sklearn, color='r', linestyle='--', label="Scikit-Learn Logistic Regression")

plt.xlabel("Iterations")
plt.ylabel("Accuracy")
plt.title("Accuracy vs. Iterations for Logistic Regression Optimization Methods")
plt.legend()
plt.show()